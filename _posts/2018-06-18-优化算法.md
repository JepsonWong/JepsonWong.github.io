---
layout: post
title: 优化算法
categories: [神经网络, 机器学习]
description: 优化算法概述
keywords: 神经网络, 机器学习, 优化算法
---

# 无约束优化问题

## 直接求导

对目标函数求导为0。

## 梯度下降优化框架

### 批量梯度下降(Batch gradient descent)

每次使用全量的训练集样本来更新模型参数。

### 随机梯度下降(Stochastic gradient descent)

当训练数据样本数很大时，梯度下降每次迭代的计算开销很高。随机梯度下降算法每次从训练集中随机选择一个样本来进行学习。因此每次的学习是非常快速的，并且可以进行在线更新。

随机梯度下降较大的缺点在于每次更新可能并不会按照正确的方向进行，因此可以带来优化波动(扰动)。

不过从另一个方面来看，随机梯度下降所带来的波动有个好处就是，对于类似盆地区域（即很多局部极小值点）那么这个波动的特点可能会使得优化的方向从当前的局部极小值点跳到另一个更好的局部极小值点，这样便可能对于非凸函数，最终收敛于一个较好的局部极值点，甚至全局极值点。 

### 小批量梯度下降(Mini-batch gradient descent)

Mini-batch梯度下降综合了batch梯度下降与stochastic梯度下降，在每次更新速度与更新次数中间取得一个平衡，其每次更新从训练集中随机选择m(m < n)个样本进行学习。

相对于随机梯度下降，Mini-batch梯度下降降低了收敛波动性，即降低了参数更新的方差，使得更新更加稳定。相对于全量梯度下降，其提高了每次学习的速度。

### 梯度下降的问题与挑战

选择一个合理的学习速率很难。如果学习速率过小，则会导致收敛速度很慢。如果学习速率过大，那么其会阻碍收敛，即在极值点附近会振荡。

学习速率调整(又称学习速率调度，Learning rate schedules)试图在**每次更新过程中，改变学习速率**，如退火。一般使用某种事先设定的策略或者在每次迭代中衰减一个较小的阈值。无论哪种调整方法，都**需要事先进行固定设置**，这边便无法自适应每次学习的数据集特点。

## 梯度下降优化算法

**更新神经网络参数的那一步骤做一些变化**。

### Momentum(动量法)

动量法使用了指数加权平均的思想。

给定目标函数，在动量法的每次迭代中，**自变量在各个方向上的移动幅度不仅取决当前梯度，还取决过去各个梯度在各个方向上是否一致**。

### Adagrad

之前介绍过的优化算法中，无论是梯度下降、随机梯度下降、小批量随机梯度下降还是使用动量法，**目标函数自变量的每一个元素**在**相同时刻**都**使用同一个学习率**来自我迭代。

调整学习率：**使得目标函数自变量中每个元素都分别拥有自己的学习率**。需要强调的是，小批量随机梯度按元素平方的累加变量s出现在含调整后学习率的梯度g′的分母项。因此，如果目标函数有关自变量中某个元素的偏导数一直都较大，那么就让该元素的学习率下降快一点（**学习率降低得快，学习率越来越小**）；反之，如果目标函数有关自变量中某个元素的偏导数一直都较小，那么就让该元素的学习率下降慢一点（**学习率降低得慢，学习率保持一个大的状态**）。然而，由于s一直在累加按元素平方的梯度，自变量中每个元素的学习率在迭代过程中一直在降低（或不变）。所以，**当学习率在迭代早期降得较快且当前解依然不佳时，Adagrad在迭代后期由于学习率过小，可能较难找到一个有用的解**。

### RMSProp

由于当学习率在迭代早期降得较快且当前解依然不佳时，Adagrad在迭代后期由于学习率过小，可能较难找到一个有用的解。RMSProp算法对Adagrad做了一点小小的修改。

RMSProp只**在Adagrad的基础上修改了变量s的更新方法**。**从累加变成了指数加权平均**。由于变量s可看作是最近1/(1−γ)个时刻的平方项g⊙g的加权平均，自变量每个元素的学习率在迭代过程中避免了“直降不升”的问题。

RMSProp和Adagrad的不同在于，RMSProp使用了小批量随机梯度按元素平方的指数加权移动平均变量来调整学习率。理解指数加权移动平均有助于我们调节RMSProp算法中的超参数，例如γ。

### Adadelta

RMSProp针对Adagrad在迭代后期可能较难找到有用解的问题，对小批量随机梯度按元素平方项做指数加权移动平均而不是累加。另一种应对该问题的优化算法叫做Adadelta。

没有学习率超参数。

### Adam

* Adam组合了动量法和RMSProp。
* Adam使用了偏差修正。

Adam是一个组合了动量法和RMSProp的优化算法。

Adam算法使用了**动量变量v**和**RMSProp中小批量随机梯度按元素平方的指数加权移动平均变量s**，并将它们中每个元素初始化为0。在每次迭代中，时刻t的小批量随机梯度记作gt。

对**变量v和s均作偏差修正**。让各个时刻权值之和为1。(β1为0.9，那么就是最近10个时刻的值之和，这个值对于动量变量v是梯度g，对于s是梯度g的平方)

## 二阶梯度下降算法

### 牛顿法

二阶收敛的优势，没有学习速率，没有超参数，只要知道方向和曲率，就能达到近似的最低值。

但在实际应用中，牛顿法不是很受欢迎，在迭代公式中的H是指Hessian矩阵，比如你有1亿的数据，Hessian矩阵是1亿行和1亿列，然后进行逆运算，Good Luck!这是不可能发生的。

### 拟牛顿法

拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它**使用正定矩阵来近似Hessian矩阵的逆**，从而简化了运算的复杂度。

#### BGFS

BGFS算法不用对Hessian矩阵进行逆运算，通过秩为1的矩阵连续更新得到一个近似的Hessian矩阵，但是依然要存储Hessian矩阵的值，所以对大型神经网络仍然不适用。

# 启发式优化方法

启发式方法指人在解决问题时所采取的一种根据经验规则进行发现的方法。其特点是在解决问题时，**利用过去的经验，选择已经行之有效的方法**，而不是系统地、以确定的步骤去寻求答案。

模拟退火方法、遗传算法、蚁群算法以及粒子群算法等等。

# 有约束优化

拉格朗日求得的并不一定是最优解，**只有在凸优化的情况下**，才能保证得到的是最优解，所以本文称拉格朗日乘子法得到的为**可行解**，其实就是局部极小值。

## 等式约束

直接利用拉格朗日乘子法求取最优值。

设目标函数**min f(x)**，等式约束h(x)。引入拉格朗日乘子u，构建新的目标函数。

满足两个条件：

* f(x)和g(x)等高线相切。在极小值点f(x)和g(x)梯度在同一条直线上。
* h(x) = 0

而构造的新的目标函数，对x求导满足上述条件1；对u求导满足上述条件2。

## 不等式约束

转化为在满足KKT约束条件下应用拉格朗日乘子法求解。

设目标函数**min f(x)**，不等式约束g(x)<=0。引入拉格朗日乘子u，构建新的目标函数。

KKT条件：

* f(x)和g(x)等高线相切。在极小值点f(x)和g(x)梯度在同一条直线上。**新的目标函数对求导**。
* u>=0
* ug(x)=0

转换为对偶问题。参照下方文章《拉格朗日对偶性》。

## 参考

[无约束优化算法——牛顿法与拟牛顿法（DFP，BFGS，LBFGS）（好文，介绍很详细）](https://www.cnblogs.com/ljy2013/p/5129294.html)

[拉格朗日乘子法和KKT条件](https://www.cnblogs.com/liaohuiqiang/p/7805954.html)

[拉格朗日对偶性](http://www.cnblogs.com/liaohuiqiang/p/7818448.html)

[约束优化方法之拉格朗日乘子法与KKT条件](约束优化方法之拉格朗日乘子法与KKT条件)

[干货|通俗理解指数加权平均](https://www.sohu.com/a/197998432_206784)

[GLUON 优化算法](http://zh.gluon.ai/chapter_optimization/index.html)

[机器学习最常用优化之一——梯度下降优化算法综述](http://www.dataguru.cn/article-10174-1.html)

[机器学习中常见的几种最优化方法](https://blog.csdn.net/owen7500/article/details/51601627)

[加速神经网络训练 (Speed Up Training)](https://morvanzhou.github.io/tutorials/machine-learning/torch/3-06-A-speed-up-learning/)

[Optimizer优化器](https://morvanzhou.github.io/tutorials/machine-learning/torch/3-06-optimizer/)
