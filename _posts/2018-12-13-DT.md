---
layout: post
title: 决策树
categories: [机器学习]
description: some word here
keywords: 机器学习, DT
---

## 决策树

决策树是一种基本的分类与回归算法。

在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是if-then规则的集合；也可以认为是定义在特征空间与类空间上的条件概率分布，将特征空间划分成了互不相交的单元。

决策树的**优点**是模型具有可读性，分类速度快。

**决策树学习通常包括三个步骤**：特征选择、决策树的生成和决策树的修剪。

决策树的算法通常是**递归地选择最优特征**，并**根据该特征对训练数据进行分割**，使得各个子数据集有一个最好的分类的过程。这一过程对应着特征空间的划分，即决策树的构建。但是决策树有可能发生过拟合现象，所以需要对已生成的树**自下而上进行剪枝**，使其有更好的泛化能力。

**决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择**。

## 特征选择

特征选择是选取对训练数据具有分类能力的特征。选取一个特征来**划分特征空间**。我们需要根据一些准则来进行特征选择。

### 信息增益（对于ID3算法）

熵是表示随机变量不确定性的度量。

设X是一个取有限值的离散随机变量，其概率分布为

<a href="https://www.codecogs.com/eqnedit.php?latex=P(X&space;=&space;x_i)&space;=&space;p_i,&space;i&space;=&space;1,2,……,n" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(X&space;=&space;x_i)&space;=&space;p_i,&space;i&space;=&space;1,2,……,n" title="P(X = x_i) = p_i, i = 1,2,……,n" /></a>

随机变量X的熵定义为

<a href="https://www.codecogs.com/eqnedit.php?latex=H(X)&space;=&space;-&space;\sum_{i=1}^{n}&space;p_ilogp_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?H(X)&space;=&space;-&space;\sum_{i=1}^{n}&space;p_ilogp_i" title="H(X) = - \sum_{i=1}^{n} p_ilogp_i" /></a>

熵只依赖于X的分布，而与X的取值无关。

<a href="https://www.codecogs.com/eqnedit.php?latex=0&space;\leq&space;H(X)&space;\leq&space;logn" target="_blank"><img src="https://latex.codecogs.com/gif.latex?0&space;\leq&space;H(X)&space;\leq&space;logn" title="0 \leq H(X) \leq logn" /></a>

条件熵H(Y\_X)表示在已知随机变量X的条件下随机变量Y的不确定性。定义为X给定条件下Y的条件概率分布的熵对X的数学期望

<a href="https://www.codecogs.com/eqnedit.php?latex=H(Y|X)&space;=&space;\sum_{i=1}^{n}&space;p_i&space;H(Y|X=x_i)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?H(Y|X)&space;=&space;\sum_{i=1}^{n}&space;p_i&space;H(Y|X=x_i)" title="H(Y|X) = \sum_{i=1}^{n} p_i H(Y|X=x_i)" /></a>

信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。

<a href="https://www.codecogs.com/eqnedit.php?latex=g(D,A)&space;=&space;H(D)&space;-&space;H(D|A)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?g(D,A)&space;=&space;H(D)&space;-&space;H(D|A)" title="g(D,A) = H(D) - H(D|A)" /></a>

**决策树学习应用信息增益准则选择特征**。信息增益大的特征具有更强的分类能力。选择方法是：对训练数据集（或子集）D，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。

### 信息增益比（对于C4.5算法）

以信息增益作为划分训练数据集的特征，**存在偏向于选择取值较多的特征的问题**，使用信息增益比可以对这个问题进行校正。

特征A对训练数据集D的信息增益比定义为信息增益与**训练数据集D关于特征A的值的熵**之比

<a href="https://www.codecogs.com/eqnedit.php?latex=g_R(D,A)&space;=&space;g(D|A)&space;/&space;H_A(D)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?g_R(D,A)&space;=&space;g(D|A)&space;/&space;H_A(D)" title="g_R(D,A) = g(D|A) / H_A(D)" /></a>

### 平方误差最小化准则、基尼指数（对于CART算法）

CART既可以用于回归，也可以用于分类。CART**假设决策树是二叉树**，内部结点特征的取值为“是”和“否”，递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布。

对于分类树，用基尼指数选择最优特征。假设有K个类，样本点属于第k类的概率为pk，则概率分布的基尼指数为

<a href="https://www.codecogs.com/eqnedit.php?latex=Gini(p)&space;=&space;\sum_{k=1}^{K}&space;p_k(1-p_k)&space;=&space;1&space;-&space;\sum_{k=1}^{K}&space;p_k{2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Gini(p)&space;=&space;\sum_{k=1}^{K}&space;p_k(1-p_k)&space;=&space;1&space;-&space;\sum_{k=1}^{K}&space;p_k{2}" title="Gini(p) = \sum_{k=1}^{K} p_k(1-p_k) = 1 - \sum_{k=1}^{K} p_k{2}" /></a>

基尼指数表示集合D的不确定性，**基尼指数越大，样本集合的不确定性越大**。

对于回归树，假设此时已经将输入空间划分为M个单元R1、R2、……、RM；并且在每个单元Rm上有一个固定的输出值cm，于是回归模型表示为

<a href="https://www.codecogs.com/eqnedit.php?latex=f(x)&space;=&space;\sum_{m=1}^{M}&space;c_mI(x\epsilon&space;R)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f(x)&space;=&space;\sum_{m=1}^{M}&space;c_mI(x\epsilon&space;R)" title="f(x) = \sum_{m=1}^{M} c_mI(x\epsilon R)" /></a>

当输入空间的划分确定时，可以用平方误差L来表示回归树对于训练数据的预测误差。**用平方误差最小的准则求解每个单元上的最优输出值**。需要找一个最优的切分点，使得平方误差最小。

<a href="https://www.codecogs.com/eqnedit.php?latex=L&space;=&space;\sum_{x_i&space;\epsilon&space;R}^{&space;}&space;(y_i&space;-&space;f(x_i)){2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?L&space;=&space;\sum_{x_i&space;\epsilon&space;R}^{&space;}&space;(y_i&space;-&space;f(x_i)){2}" title="L = \sum_{x_i \epsilon R}^{ } (y_i - f(x_i)){2}" /></a>

## 决策树的生成

### ID3算法

ID3算法的核心在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。

* 从根结点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征
* 由该特征的不同值建立子结点，对子结点递归调用以上方法，构建决策树
* 结束条件是：直到所有特征的信息增益均很小或没有特征可以选择为止

### C4.5生成算法

C4.5算法与ID3算法类似，对ID3算法进行了改进。

* 如果D中所有实例都属于同一类或者特征不存在，则决策树为单结点树。将D中实例数最大的类作为该结点的类
* 按照信息增益比选择该值大的特征。如果信息增益比小于阈值，则停止分裂；否则，按照信息增益比递归分裂

### CART生成算法

#### 回归树的生成

在训练数据集所在的输入空间中，递归地将每个区域划分为两个自区域并决定每个子区域上的输出值，构造二叉决策树。

* 选择最优切分变量与切分点，选择准则是平方误差
* 用选定的切分变量和切分点来划分区域并决定相应的输入值，输出值是该结点下所有数据的平均值
* 递归调用以上步骤，生成决策树

#### 分类树的生成

* 选择最优的切分特征和切分值，计算基尼指数，选择基尼指数最小的特征及其切分值作为最优特征与最优切分值
* 根据选定的切分特征和切分值来划分数据
* 递归调用以上步骤，生成决策树；算法结束条件是没有更多特征或者结点中样本个数小于阈值或者基尼指数小于阈值

## 决策树的剪枝

ID3算法只有树的生成，没有树的剪枝，所以**容易产生过拟合**。

### 预剪枝

决策树生成过程中进行，会判断该结点的划分是否能带来决策树泛化性能的提升，如果不能，则该结点停止分裂。

### 后剪枝

先生成一颗完整的决策树，然后自底向上剪枝。

### CART剪枝算法

CART剪枝算法从“完全生长”的决策树的底端剪去一些子树，使决策树变小。

* 首先从决策树底端开始不断剪枝，直到根结点，形成一系列的子树序列。
* 然后通过交叉验证发在**独立的验证集上**对子树序列进行测试，从中选择最优子树。

剪枝的过程主要是给定一个loss，这个loss是在训练数据的预测误差的基础上**加一个叶子结点的正则化参数**。每次随机去一个叶子结点，选择使得损失函数最小的子数。这样，**逐步选出子数序列**。

## 连续与缺失值处理

### 连续值处理

最简单的策略是采用二分法对连续值进行处理。这正是C4.5决策树算法采用的机制。还有就是对连续数据进行离散化，分为几个值，然后当作离散值进行处理。

### 缺失值处理

这里存在两个问题：

* 如何在属性值缺失的情况下进行划分属性选择
* 给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分

对于第一个问题，若取值未知，则根据其他样本的取值来计算划分点。

对于第二个问题，若取值未知，则将该样本同时划入所有子结点，且设置一个样本权值用于计算loss。

## sklearn库

### 决策树参数

DecisionTreeClassifier

* 特征选择标准criterion
* 特征划分点选择标准splitter
* 划分时考虑的最大特征数max\_features
* 决策树最大深max\_depth
* 内部节点再划分所需最小样本数min\_samples\_split
* 叶子节点最少样本数min\_samples\_leaf
* 叶子节点最小的样本权重和min\_weight\_fraction\_leaf
* 最大叶子节点数max\_leaf\_nodes
* 类别权重class\_weight
* 节点划分最小不纯度min\_impurity\_split

### Python绘制决策树

scikit-learn中决策树的可视化一般需要安装graphviz。主要包括graphviz的安装和python的graphviz插件的安装。

用graphviz的dot命令生成决策树的可视化文件，敲完这个命令后当前目录就可以看到决策树的可视化文件iris.pdf.打开可以看到决策树的模型图。

## 参考

[机器学习西瓜书]()

[统计学习方法]()

[scikit-learn决策树算法类库使用小结](https://www.cnblogs.com/pinard/p/6056319.html)
