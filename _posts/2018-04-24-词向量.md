---
layout: post
title: 词向量模型
categories: [NLP, 神经网络]
description: 介绍一些词向量模型。
keywords: NLP, 词向量
---

## 1. Word2Vec

[一文详解 Word2vec 之 Skip-Gram 模型（结构篇）](https://www.leiphone.com/news/201706/PamWKpfRFEI42McI.html)

[一文详解 Word2vec 之 Skip-Gram 模型（训练篇）](https://www.leiphone.com/news/201706/eV8j3Nu8SMqGBnQB.html)

[一文详解 Word2vec 之 Skip-Gram 模型（实现篇）](https://www.leiphone.com/news/201706/QprrvzsrZCl4S2lw.html)

## 2. GloVe

GloVe(Global Vectors for Word Representation)是一种基于共现矩阵分解的词向量。

[Glove词向量](https://nlp.stanford.edu/projects/glove/)

[NLP︱高级词向量表达（一）——GloVe（理论、相关测评结果、R&python实现、相关应用）](https://blog.csdn.net/sinat_26917383/article/details/54847240)

## 3. WordRank

[NLP︱高级词向量表达（三）——WordRank（简述）](https://blog.csdn.net/sinat_26917383/article/details/54852214)

## 4. Polyglot

[Polyglot，里面有英文词向量、中文词向量、字向量](https://sites.google.com/site/rmyeid/projects/polyglot#TOC-Abstract)

[如何读取Polyglot词向量文件](http://nbviewer.jupyter.org/gist/aboSamoor/6046170)

## 5. fastText

[fastText预训练的157种语言的词向量](https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md)

[NLP︱高级词向量表达（二）——FastText（简述、学习笔记）](https://blog.csdn.net/sinat_26917383/article/details/54850933)

## github资源
[word2vec-api](https://github.com/3Top/word2vec-api)

[Pre-trained word vectors of 30+ languages，里面有中文word2vec和fastText的词向量](https://github.com/Kyubyong/wordvectors)

## 博客资源
注意：训练好的word2vec和glove词向量均可以用gensim导入。

[Google's trained Word2Vec model in Python](http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/)

[Word2Vec Resources](http://mccormickml.com/2016/04/27/word2vec-resources/)

[60维训练好的中文词向量-gensim版本要对应，要不然没法用！](https://www.cnblogs.com/Darwin2000/p/5786984.html)

[Gensim Word2vec 使用指南](Gensim Word2vec 使用指南)

[python 环境下gensim中的word2vec的使用笔记](https://blog.csdn.net/philosophyatmath/article/details/52354413)

[gensim词向量之加载word2vec和glove](https://blog.csdn.net/u010041824/article/details/70832295)

[Gensim Word2vec简介](https://blog.csdn.net/lixintong1992/article/details/51607372)

[Keras模型中使用预训练的词向量](http://www.360doc.com/content/17/0126/23/40028542_624946612.shtml)

[玩转Fasttext](http://albertxiebnu.github.io/fasttext/)

## 论文

### Mikolov的原始论文

《DistributedRepresentations of Words and Phrases and their Compositionality》

《Efficient Estimation ofWord Representations in Vector Space》

### DL在language model中应用的祖先文章（2003年）

《A Neural ProbabilisticLanguage Model》 by Yoshua Bengio et. al.

### Word2Vec在文章种被解释为一种变种PCA

《Neural Word Embedding asMatrix Factorization》by Yoav Goldberg et. al.

### 以unsupervised的方式产生embedding

Stanford的Chris Manning的 Glove：《Global Vectors for WordRepresentation》
