---
layout: post
title: 词向量模型
categories: [NLP, 神经网络]
description: 介绍一些词向量模型。
keywords: NLP, 词向量
---

下面链接里面也有很多相关的词向量模型，但是有些其实不太常用。以下总结了一些比较常用的一些词向量的相关博文和实战代码。

[awesome-sentence-embedding](https://github.com/Separius/awesome-sentence-embedding)

[当前最好的词句嵌入技术概览：从无监督学习转向监督、多任务学习](https://zhuanlan.zhihu.com/p/37761272)

## 1. Word Embedding

### 1.1 Word2Vec

[一文详解 Word2vec 之 Skip-Gram 模型（结构篇）](https://www.leiphone.com/news/201706/PamWKpfRFEI42McI.html)

[一文详解 Word2vec 之 Skip-Gram 模型（训练篇）](https://www.leiphone.com/news/201706/eV8j3Nu8SMqGBnQB.html)

[一文详解 Word2vec 之 Skip-Gram 模型（实现篇）](https://www.leiphone.com/news/201706/QprrvzsrZCl4S2lw.html)

基于分布假设（在相同的上下文中出现的单词往往具有相似的含义）的无监督学习方法

### 1.2 GloVe

GloVe(Global Vectors for Word Representation)是一种基于共现矩阵分解的词向量。

[Glove词向量](https://nlp.stanford.edu/projects/glove/)

[NLP︱高级词向量表达（一）——GloVe（理论、相关测评结果、R&python实现、相关应用）](https://blog.csdn.net/sinat_26917383/article/details/54847240)

基于分布假设（在相同的上下文中出现的单词往往具有相似的含义）的无监督学习方法

### 1.3 WordRank

[NLP︱高级词向量表达（三）——WordRank（简述）](https://blog.csdn.net/sinat_26917383/article/details/54852214)

### 1.4 Polyglot

[Polyglot，里面有英文词向量、中文词向量、字向量](https://sites.google.com/site/rmyeid/projects/polyglot#TOC-Abstract)

[如何读取Polyglot词向量文件](http://nbviewer.jupyter.org/gist/aboSamoor/6046170)

### 1.5 fastText

[fastText预训练的157种语言的词向量](https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md)

[NLP︱高级词向量表达（二）——FastText（简述、学习笔记）](https://blog.csdn.net/sinat_26917383/article/details/54850933)

有监督 + ngram（可以解决OOV问题） + 速度快

## 2. Contextualized Word Embedding

[从Word Embedding到Bert模型-自然语言处理中的预训练技术发展史(理论+实践)](https://zhuanlan.zhihu.com/p/54448555)

### 2.1 Transformer

Transformer代码分析：https://github.com/JepsonWong/Transformer

### 2.2 ELMO

代码很清晰，可读性很好。https://github.com/codertimo/ELMO-tf

注意：ELMo模型的输入是字符而不是单词。

### 2.3 BERT

BERT代码实战（中英文文本分类）：https://github.com/JepsonWong/BERT

## 3. 其他

### 3.1 基于笔画的中文词的word embedding

cw2vec

[cw2vec理论及其实现](https://blog.csdn.net/quxing10086/article/details/80332538) 

## 4. 词嵌入模型的最优维度

[NeurIPS 2018 oral论文解读：如何给词嵌入模型选择最优维度](https://zhuanlan.zhihu.com/p/53958685)

里面有**各个词向量的最优维度**以及最优维度的选择原理。

## github资源
[word2vec-api](https://github.com/3Top/word2vec-api)

[Pre-trained word vectors of 30+ languages，里面有中文word2vec和fastText的词向量](https://github.com/Kyubyong/wordvectors)

## 博客资源

注意：训练好的word2vec和glove词向量均可以用gensim导入。

[Google's trained Word2Vec model in Python](http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/)

[Word2Vec Resources](http://mccormickml.com/2016/04/27/word2vec-resources/)

[60维训练好的中文词向量-gensim版本要对应，要不然没法用！](https://www.cnblogs.com/Darwin2000/p/5786984.html)

[Gensim Word2vec 使用指南](Gensim Word2vec 使用指南)

[python 环境下gensim中的word2vec的使用笔记](https://blog.csdn.net/philosophyatmath/article/details/52354413)

[gensim词向量之加载word2vec和glove](https://blog.csdn.net/u010041824/article/details/70832295)

[Gensim Word2vec简介](https://blog.csdn.net/lixintong1992/article/details/51607372)

[Keras模型中使用预训练的词向量](http://www.360doc.com/content/17/0126/23/40028542_624946612.shtml)

[玩转Fasttext](http://albertxiebnu.github.io/fasttext/)

## 论文

### Mikolov的原始论文

《DistributedRepresentations of Words and Phrases and their Compositionality》

《Efficient Estimation ofWord Representations in Vector Space》

### DL在language model中应用的祖先文章（2003年）

《A Neural ProbabilisticLanguage Model》 by Yoshua Bengio et. al.

### Word2Vec在文章种被解释为一种变种PCA

《Neural Word Embedding asMatrix Factorization》by Yoav Goldberg et. al.

### 以unsupervised的方式产生embedding

Stanford的Chris Manning的 Glove：《Global Vectors for WordRepresentation》
