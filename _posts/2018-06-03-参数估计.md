---
layout: post
title: 参数估计
categories: [机器学习]
description: 介绍统计学中的参数估计的方法。
keywords: 机器学习, 参数估计
---

## 最大似然估计(MLE)

最大似然估计简单来说就是模型确定，参数未知。**不考虑先验概率P(Yi)**，因此属于频率学派。就是已知模型的前提下，**似然函数（概率)最大**时对应的参数。

最大似然估计中采样需满足一个很重要的假设，就是所有的**采样都是独立同分布的**。

最大似然估计的一般求解过程：

* 写出似然函数；
* 对似然函数取对数，并整理；
* 求导数；
* 解似然方程。
* 注意：**最大似然估计只考虑某个模型能产生某个给定观察序列的概率。而未考虑该模型本身的概率即先验概率**。这点与贝叶斯估计区别。

## 最大后验估计(MAP)

最大后验估计是根据经验数据获得对难以观察的量的点估计。与最大似然估计类似，但是最大的不同时，最大后验估计的**融入了要估计量的先验分布在其中**。故最大后验估计可以看做规则化的最大似然估计。

注：**贝叶斯要考虑其先验概率，即需要在似然函数后乘以其先验概率**。

## 贝叶斯估计

贝叶斯估计把参数W看成一个随机变量，通过后验分布p(W|D)来计算参数W在条件D下的数学期望，即加权使用所有参数W，而权重由后验分布p(W|D)确定（类似于离散情形的加权平均），从而起到分摊估计参数W的不确定性。这是比极大似然估计和最大后验估计这种点估计先进的地方。

## 最小二乘法(LSM)

最小二乘法（Least Square）

最小二乘法的本质就是找到一个估计值，使实际值与估计值的距离最小。而为了度量最小距离，只要使实际值与估计值之差的平方最小就好，下面就是最小二乘的表达式损失函数cost function，我们的目标就是求θ。

求解方法是通过梯度下降算法，通过训练数据不断迭代得到最终的值。最小二乘的主要应用场景为回归分析，因为回归常用平方损失作为损失函数。

## 最大似然估计与最大后验估计区别与联系

在MAP中我们应注意的是：

**MAP与MLE最大区别是MAP中加入了模型参数本身的概率分布，即是否考虑了先验知识**。或者说，MLE中认为模型参数本身的概率的是均匀的，即该概率为一个固定值。

最大后验概率和极大似然估计很像，只是多了一项先验分布，**它体现了贝叶斯认为参数也是随机变量的观点**，在实际运算中通常通过超参数给出先验分布。

从以上可以看出，一方面，极大似然估计和最大后验概率都是参数的点估计。在频率学派中，参数固定了，预测值也就固定了。最大后验概率是贝叶斯学派的一种近似手段，因为完全贝叶斯估计不一定可行。另一方面，最大后验概率可以看作是对先验和MLE的一种折衷，如果数据量足够大，最大后验概率和最大似然估计趋向于一致，如果数据为0，最大后验仅由先验决定。

## 最大似然估计与最大后验估计与贝叶斯估计的区别与联系

当先验为均匀分布时，极大似然估计和最大后验估计是等价的。

当先验和似然都是高斯分布时，最大后验估计和贝叶斯估计是等价的。

## 参考

[机器学习->统计学基础->贝叶斯估计,最大似然估计(MLE),最大后验估计(MAP)](https://blog.csdn.net/Mr_tyting/article/details/62882162)

[最大似然估计 （MLE）与 最大后验概率（MAP）在机器学习中的应用](https://blog.csdn.net/shenxiaoming77/article/details/51643845)

[机器学习理论：贝叶斯估计](http://baijiahao.baidu.com/s?id=1593989727950148259&wfr=spider&for=pc)

[极大似然估计与贝叶斯估计](https://blog.csdn.net/liu1194397014/article/details/52766760)

[最小二乘、最大似然和最大后验的简单总结](https://blog.csdn.net/woaidapaopao/article/details/53174749)

[机器学习中的MLE、MAP、贝叶斯估计](https://blog.csdn.net/perfectlwz/article/details/80487346)
