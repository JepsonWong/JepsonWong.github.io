---
layout: post
title: Word2Vec
categories: [NLP, 神经网络]
description: 总结Word2Vec
keywords: NLP, 神经网络
---

## 契机

写这篇文章的契机在于最近面试总是被问到Word2Vec的相关知识，包括基本的理论、参数意义、实现trick等。

## 原理

Word2Vec作为神经概率语言模型的输入，其本身其实是神经概率模型的副产品，是为了通过神经网络学习某个语言模型而产生的中间结果。具体来说，某个语言模型指的是“CBOW”和“Skip-gram”。具体学习过程会用到两个降低复杂度的近似方法-Hierarchical Softmax或Negative Sampling。**两个模型**乘以**两种方法**，一共有四种实现。

Word2Vec得到**以该词作为背景词和中心词的两组词向量**。我们**会使用连续词袋模型的背景词向量，使用跳字模型的中心词向量**。

### 预备知识

sigmoid函数

逻辑回归

Bayes公式：语言模型P(Text)，声学模型P(Voice\|Text)，语音识别P(Text\|Voice)由前两个模型利用Bayes公式推导出来。

Huffman编码：Huffman树、Huffman树的构造、Huffman编码

### 语言模型

什么是统计语言模型呢？通俗地说，**统计语言模型描述了一串文字序列成为句子的概率**。

n-gram语言模型：模型参数很多。

神经概率语言模型：输入层、隐藏层、输出层。**词向量只是该语言模型的一个副产品**。

### 基于Hierarchical Softmax模型

CBOW模型：包含输入层、投影层和输出层。输入层包含2c个词向量；投影层将输入层词向量做求和累加；输出层是一个**二叉树**，词作为叶子节点，构造**Huffman树**。

Skip-gram模型：输入层是一个词向量，投影层也是一个和输入层一样的词向量，输出层是一个**Huffman树**。

### 基于Negative Sampling的模型

负采样。增大正样本的概率同时降低负样本的概率。

## Word2Vec的参数

有一些参数设置对Word2Vec很重要，以及Word2Vec的一些trick。

窗口设置一般是5，而且是左右随机1-5的大小，是均匀分布。为什么选择随机窗口呢？目的是增加随机性。窗口设置表示当前词与预测词在一个句子中的最大距离是多少。

词向量维度。

负采样很重要，一般设置为10，如果设置为更大会有小量提升。

## 参考

[word2vec原理(二) 基于Hierarchical Softmax的模型](http://www.cnblogs.com/pinard/p/7243513.html)

[word2vec 中的数学原理详解（二）预备知识](https://blog.csdn.net/itplus/article/details/37969635)

[浅析自然语言理解之统计语言模型](https://www.sohu.com/a/115750211_491255)

[word2vec原理推导与代码分析](http://www.hankcs.com/nlp/word2vec.html)
