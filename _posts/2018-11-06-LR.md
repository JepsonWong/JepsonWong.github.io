---
layout: post
title: 逻辑回归
categories: [机器学习]
description: some word here
keywords: 机器学习
---

## 介绍

用于处理分类任务

### 对数几率

对数几率函数（logistic function）：

<a href="https://www.codecogs.com/eqnedit.php?latex=y&space;=&space;\frac{1}{1&space;&plus;&space;e^{-z}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y&space;=&space;\frac{1}{1&space;&plus;&space;e^{-z}}" title="y = \frac{1}{1 + e^{-z}}" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=ln&space;\frac{y}{1&space;-&space;y}&space;=&space;z&space;=&space;w^Tx&space;&plus;&space;b" target="_blank"><img src="https://latex.codecogs.com/gif.latex?ln&space;\frac{y}{1&space;-&space;y}&space;=&space;z&space;=&space;w^Tx&space;&plus;&space;b" title="ln \frac{y}{1 - y} = z = w^Tx + b" /></a>

几率：若将y看做样本x为正例的可能性，则1-y为样本x为反例的可能性，这两者的比例称为几率。

对数几率：将上述几率取对数。

对数几率回归：上式实际上是**用线性回归模型的预测结果去逼近真实标记的对数几率**，因此，其对应的模型称为对数几率回归（logistic regression）。它是一种分类学习方法。

对数几率回归的优点：

* 它直接对分类可能性进行建模，无需事先假设数据分布，避免了假设分布不准确所带来的问题。
* 它不仅预测出类别，还得到近似概率预测，对许多需利用概率辅助决策的任务很有用。
* 对率函数是任意阶可导的凸函数，有很好的数学性质，现有的许多数值优化算法都可直接用于求取最优解。

## 逻辑回归

### 模型: 逻辑回归函数模型

<a href="https://www.codecogs.com/eqnedit.php?latex=h_\Theta&space;(x)&space;=&space;\frac{1}{1&space;&plus;&space;e^{-\Theta^Tx&space;}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h_\Theta&space;(x)&space;=&space;\frac{1}{1&space;&plus;&space;e^{-\Theta^Tx&space;}}" title="h_\Theta (x) = \frac{1}{1 + e^{-\Theta^Tx }}" /></a>

### 策略: 损失函数

如果用线性回归的代价函数，

<a href="https://www.codecogs.com/eqnedit.php?latex=J(\Theta&space;)&space;=&space;\frac{1}{m}\sum_{i&space;=&space;1}^{m}\frac{1}{2}(h_\Theta&space;(x^{(i)})&space;-&space;y^{(i)})^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?J(\Theta&space;)&space;=&space;\frac{1}{m}\sum_{i&space;=&space;1}^{m}\frac{1}{2}(h_\Theta&space;(x^{(i)})&space;-&space;y^{(i)})^2" title="J(\Theta ) = \frac{1}{m}\sum_{i = 1}^{m}\frac{1}{2}(h_\Theta (x^{(i)}) - y^{(i)})^2" /></a>

这样的话代价函数会非常复杂，会存在多个局部最小值，即非凸的。而我们想要代价函数是凸函数，这样我们可以很容易的找出全局最优解。所以我们采用**最大似然估计**来估计参数。

逻辑回归的似然函数如下

<a href="https://www.codecogs.com/eqnedit.php?latex=l(\Theta&space;)&space;=&space;\prod_{i=&space;1}^{m}p(y&space;=&space;1|&space;x_i)^{y_i}p(y&space;=&space;0&space;|&space;x_i)^{1-y_i}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?l(\Theta&space;)&space;=&space;\prod_{i=&space;1}^{m}p(y&space;=&space;1|&space;x_i)^{y_i}p(y&space;=&space;0&space;|&space;x_i)^{1-y_i}" title="l(\Theta ) = \prod_{i= 1}^{m}p(y = 1| x_i)^{y_i}p(y = 0 | x_i)^{1-y_i}" /></a>

对数似然函数如下

<a href="https://www.codecogs.com/eqnedit.php?latex=l(\Theta&space;)&space;=&space;\sum_{i&space;=&space;1}^{m}[y_ilnp(y&space;=&space;1&space;|&space;x_i)&space;&plus;&space;(1&space;-&space;y_i)lnp(y&space;=&space;0&space;|&space;x_i)]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?l(\Theta&space;)&space;=&space;\sum_{i&space;=&space;1}^{m}[y_ilnp(y&space;=&space;1&space;|&space;x_i)&space;&plus;&space;(1&space;-&space;y_i)lnp(y&space;=&space;0&space;|&space;x_i)]" title="l(\Theta ) = \sum_{i = 1}^{m}[y_ilnp(y = 1 | x_i) + (1 - y_i)lnp(y = 0 | x_i)]" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=l(\Theta&space;)&space;=&space;\sum_{i&space;=&space;1}^{m}[y_ilnh_\Theta&space;x_i&space;&plus;&space;(1&space;-&space;y_i)ln(1&space;-&space;h_\Theta&space;x_i)]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?l(\Theta&space;)&space;=&space;\sum_{i&space;=&space;1}^{m}[y_ilnh_\Theta&space;x_i&space;&plus;&space;(1&space;-&space;y_i)ln(1&space;-&space;h_\Theta&space;x_i)]" title="l(\Theta ) = \sum_{i = 1}^{m}[y_ilnh_\Theta x_i + (1 - y_i)ln(1 - h_\Theta x_i)]" /></a>

即令每个样本属于其真实标记的概率越大越好，<a href="https://www.codecogs.com/eqnedit.php?latex=l(\Theta&space;)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?l(\Theta&space;)" title="l(\Theta )" /></a>是高阶连续可导的凸函数，由凸优化理论可以根据梯度下降、牛顿法等求最优解。

逻辑回归的代价函数为

<a href="https://www.codecogs.com/eqnedit.php?latex=J(\Theta&space;)&space;=&space;-\frac{1}{m}[\sum_{i&space;=&space;1}^{m}y^{(i)}lnh_\Theta&space;(x^{(i)})&space;&plus;&space;(1&space;-&space;y^{(i)})ln(1&space;-&space;h_\Theta&space;(x^{(i)}))]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?J(\Theta&space;)&space;=&space;-\frac{1}{m}[\sum_{i&space;=&space;1}^{m}y^{(i)}lnh_\Theta&space;(x^{(i)})&space;&plus;&space;(1&space;-&space;y^{(i)})ln(1&space;-&space;h_\Theta&space;(x^{(i)}))]" title="J(\Theta ) = -\frac{1}{m}[\sum_{i = 1}^{m}y^{(i)}lnh_\Theta (x^{(i)}) + (1 - y^{(i)})ln(1 - h_\Theta (x^{(i)}))]" /></a>

它是由极大似然得到的。

### 算法: 梯度下降 

由梯度下降

<a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;_j&space;:=&space;\Theta&space;_j&space;-&space;\eta&space;\sum_{i&space;=&space;1}^{m}(h_\Theta&space;(x^{(i)})&space;-&space;y^{(i)})x_j^{(i)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta&space;_j&space;:=&space;\Theta&space;_j&space;-&space;\eta&space;\sum_{i&space;=&space;1}^{m}(h_\Theta&space;(x^{(i)})&space;-&space;y^{(i)})x_j^{(i)}" title="\Theta _j := \Theta _j - \eta \sum_{i = 1}^{m}(h_\Theta (x^{(i)}) - y^{(i)})x_j^{(i)}" /></a>

## 一些trick

### 过拟合问题

源自过多特征。

* 减少特征数量。
* 正则化。保留所有特征，但减少特征权重。

### 其他优化算法

* Conjugate gradient method(共轭梯度法)
* Quasi-Newton method(拟牛顿法)
* BFGS method
* L-BFGS(Limited-memory BFGS)

后二者由拟牛顿法引申出来，与梯度下降算法相比，这些算法的优点是：

* 第一，不需要手动的选择步长；
* 第二，通常比梯度下降算法快；
* 但是缺点是更复杂。

### 多分类问题

分解为多个二分类问题。

一对一、一对多或者多对多。

## 参考

[机器学习总结之逻辑回归Logistic Regression](https://blog.csdn.net/hustlx/article/details/51153029)

[代码实现](https://github.com/SmirkCao/MLiA/blob/master/CH05%20Logistic%20Regression.ipynb)
