---
layout: post
title: 逻辑回归
categories: [机器学习]
description: some word here
keywords: 机器学习
---

## 介绍

用于处理分类任务。

我们将线性回归与逻辑回归进行对比，可以发现线性回归模型在训练时在整个实数域上对异常点的敏感性是一致的（说白了就是只要是异常，就是0判断成了1，1判断成了0；而不是0判断成了0.2，1判断成了0.8这种），因而在处理分类问题时线性回归模型的效果较差，线性回归模型不适合处理分类问题。对于二分类任务，逻辑回归输出标记y = {0, 1}，而线性回归模型产生的预测值y是实值，所以我们需要一个映射函数将实值转换为0/1值。

最理想的映射函数是单位阶跃函数，即预测值大于零就判为正例，预测值小于零则判为负例，预测值为临界值则可任意判别。虽然单位阶跃函数看似完美解决了这个问题，但是**单位阶跃函数不连续并且不充分光滑**，因而无法进行求解。

所以我们希望找到一个近似函数来替代单位阶跃函数，并希望它**单调可微**。对数概率函数正是这样一个替代的函数。对数概率函数将y的值转化为接近0或1的值，并且其输出y=0处变化很抖。

### 对数几率

对数几率函数（logistic function）：

<a href="https://www.codecogs.com/eqnedit.php?latex=y&space;=&space;\frac{1}{1&space;&plus;&space;e^{-z}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y&space;=&space;\frac{1}{1&space;&plus;&space;e^{-z}}" title="y = \frac{1}{1 + e^{-z}}" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=ln&space;\frac{y}{1&space;-&space;y}&space;=&space;z&space;=&space;w^Tx&space;&plus;&space;b" target="_blank"><img src="https://latex.codecogs.com/gif.latex?ln&space;\frac{y}{1&space;-&space;y}&space;=&space;z&space;=&space;w^Tx&space;&plus;&space;b" title="ln \frac{y}{1 - y} = z = w^Tx + b" /></a>

几率：若将y看做样本x为正例的可能性，则1-y为样本x为反例的可能性，这两者的比例称为几率。

对数几率：将上述几率取对数。

对数几率回归：上式实际上是**用线性回归模型的预测结果去逼近真实标记的对数几率**，因此，其对应的模型称为对数几率回归（logistic regression）。它是一种分类学习方法。

对数几率回归的优点：

* 它直接对分类可能性进行建模，无需事先假设数据分布，避免了假设分布不准确所带来的问题。
* 它不仅预测出类别，还得到近似概率预测，对许多需利用概率辅助决策的任务很有用。
* 对率函数是任意阶可导的凸函数，有很好的数学性质，现有的许多数值优化算法都可直接用于求取最优解。

## 逻辑回归

### 模型: 逻辑回归函数模型

<a href="https://www.codecogs.com/eqnedit.php?latex=h_\Theta&space;(x)&space;=&space;\frac{1}{1&space;&plus;&space;e^{-\Theta^Tx&space;}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h_\Theta&space;(x)&space;=&space;\frac{1}{1&space;&plus;&space;e^{-\Theta^Tx&space;}}" title="h_\Theta (x) = \frac{1}{1 + e^{-\Theta^Tx }}" /></a>

### 策略: 损失函数

如果用线性回归的代价函数，

<a href="https://www.codecogs.com/eqnedit.php?latex=J(\Theta&space;)&space;=&space;\frac{1}{m}\sum_{i&space;=&space;1}^{m}\frac{1}{2}(h_\Theta&space;(x^{(i)})&space;-&space;y^{(i)})^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?J(\Theta&space;)&space;=&space;\frac{1}{m}\sum_{i&space;=&space;1}^{m}\frac{1}{2}(h_\Theta&space;(x^{(i)})&space;-&space;y^{(i)})^2" title="J(\Theta ) = \frac{1}{m}\sum_{i = 1}^{m}\frac{1}{2}(h_\Theta (x^{(i)}) - y^{(i)})^2" /></a>

这样的话代价函数会非常复杂，会存在多个局部最小值，即非凸的。而我们想要代价函数是凸函数，这样我们可以很容易的找出全局最优解。所以我们采用**最大似然估计**来估计参数。

逻辑回归的似然函数如下

<a href="https://www.codecogs.com/eqnedit.php?latex=l(\Theta&space;)&space;=&space;\prod_{i=&space;1}^{m}p(y&space;=&space;1|&space;x_i)^{y_i}p(y&space;=&space;0&space;|&space;x_i)^{1-y_i}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?l(\Theta&space;)&space;=&space;\prod_{i=&space;1}^{m}p(y&space;=&space;1|&space;x_i)^{y_i}p(y&space;=&space;0&space;|&space;x_i)^{1-y_i}" title="l(\Theta ) = \prod_{i= 1}^{m}p(y = 1| x_i)^{y_i}p(y = 0 | x_i)^{1-y_i}" /></a>

对数似然函数如下

<a href="https://www.codecogs.com/eqnedit.php?latex=l(\Theta&space;)&space;=&space;\sum_{i&space;=&space;1}^{m}[y_ilnp(y&space;=&space;1&space;|&space;x_i)&space;&plus;&space;(1&space;-&space;y_i)lnp(y&space;=&space;0&space;|&space;x_i)]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?l(\Theta&space;)&space;=&space;\sum_{i&space;=&space;1}^{m}[y_ilnp(y&space;=&space;1&space;|&space;x_i)&space;&plus;&space;(1&space;-&space;y_i)lnp(y&space;=&space;0&space;|&space;x_i)]" title="l(\Theta ) = \sum_{i = 1}^{m}[y_ilnp(y = 1 | x_i) + (1 - y_i)lnp(y = 0 | x_i)]" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=l(\Theta&space;)&space;=&space;\sum_{i&space;=&space;1}^{m}[y_ilnh_\Theta&space;x_i&space;&plus;&space;(1&space;-&space;y_i)ln(1&space;-&space;h_\Theta&space;x_i)]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?l(\Theta&space;)&space;=&space;\sum_{i&space;=&space;1}^{m}[y_ilnh_\Theta&space;x_i&space;&plus;&space;(1&space;-&space;y_i)ln(1&space;-&space;h_\Theta&space;x_i)]" title="l(\Theta ) = \sum_{i = 1}^{m}[y_ilnh_\Theta x_i + (1 - y_i)ln(1 - h_\Theta x_i)]" /></a>

即令每个样本属于其真实标记的概率越大越好，<a href="https://www.codecogs.com/eqnedit.php?latex=l(\Theta&space;)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?l(\Theta&space;)" title="l(\Theta )" /></a>是高阶连续可导的凸函数，由凸优化理论可以根据梯度下降、牛顿法等求最优解。

逻辑回归的代价函数为

<a href="https://www.codecogs.com/eqnedit.php?latex=J(\Theta&space;)&space;=&space;-\frac{1}{m}[\sum_{i&space;=&space;1}^{m}y^{(i)}lnh_\Theta&space;(x^{(i)})&space;&plus;&space;(1&space;-&space;y^{(i)})ln(1&space;-&space;h_\Theta&space;(x^{(i)}))]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?J(\Theta&space;)&space;=&space;-\frac{1}{m}[\sum_{i&space;=&space;1}^{m}y^{(i)}lnh_\Theta&space;(x^{(i)})&space;&plus;&space;(1&space;-&space;y^{(i)})ln(1&space;-&space;h_\Theta&space;(x^{(i)}))]" title="J(\Theta ) = -\frac{1}{m}[\sum_{i = 1}^{m}y^{(i)}lnh_\Theta (x^{(i)}) + (1 - y^{(i)})ln(1 - h_\Theta (x^{(i)}))]" /></a>

它是由极大似然得到的。

### 算法: 梯度下降 

由梯度下降

<a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;_j&space;:=&space;\Theta&space;_j&space;-&space;\eta&space;\sum_{i&space;=&space;1}^{m}(h_\Theta&space;(x^{(i)})&space;-&space;y^{(i)})x_j^{(i)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta&space;_j&space;:=&space;\Theta&space;_j&space;-&space;\eta&space;\sum_{i&space;=&space;1}^{m}(h_\Theta&space;(x^{(i)})&space;-&space;y^{(i)})x_j^{(i)}" title="\Theta _j := \Theta _j - \eta \sum_{i = 1}^{m}(h_\Theta (x^{(i)}) - y^{(i)})x_j^{(i)}" /></a>

## 模型评估

对于LR分类模型，一般使用AUC来进行评估。

### AUC

AUC即ROC曲线下的面积，ROC曲线纵坐标是true positive rate（实际是1中，猜对多少），横坐标是false positive rate（实际是0中，猜错多少）。

AUC值范围\[0.5, 1\]。直观含义是任意取一个正样本和负样本，正样本得分大于负样本的概率。

意义：

* 通过ROC曲线，也能够在查全率和查准率之间做一个平衡，分类时候来选择出最好的阈值；
* 即使不需要二分类选阈值，比如LR回归，不把阈值作为点击或者不点击的区分标准，而是作为排序的一个因子，AUC也能够起到很好的参考意义。**AUC面积越大，说明算法和模型准确率越高越好**。
* 在比较两种学习器的时候，如果一个被另一个完全包住，那么大的那个好。如果两个有交叉，一般而言，面积大的那个好。当然不排除有具体的案例，需要根据代价敏感性（对于查全查准）的特殊需求，来比较。
* 对于**样本不平衡，AUC值不受影响**。

### Accuracy、Precision、Recall、F1-Measure

准确率：Accuracy = (TP+TN)/(P+N)，表示预测正确的样本占总样本的比例。

精准率：Precision = TP/(TP+FP)，表示预测为1的样本有多少预测正确。

召回率：Recall = TP/(TP+FN)，表示本来为1的样本中有多少比例预测为1。

F1值就是精确率和召回率的调和均值，可以通过参数调节你最看重哪一部分。

### 计算

**最简单的计算方法**是得到一系列样本被划分为正类的概率，然后按照大小排序；接下来，我们从高到低，依次将“Score”值作为阈值threshold，当测试样本属于正样本的概率大于或等于这个threshold时，我们认为它为正样本，否则为负样本；每次选取一个不同的threshold，我们就可以得到一组FPR和TPR，即ROC曲线上的一点。这样我们一共得到了n组FPR和TPR的值，将它们画在曲线上，然后计算曲线下方的面积（n为样本个数）。

但上述计算方法太复杂，所以利用一个等价关系来计算。具体来说就是统计一下所有的M×N(M为正类样本的数目，N为负类样本的数目)个正负样本对中，有多少个组中的正样本的score大于负样本的score。当二元组中正负样本的score相等的时候，按照0.5计算；然后除以MN。实现这个方法的复杂度为O(n^2)。n为样本数（即n=M+N）。

```
from sklearn import cross_validation,metrics

# 求验证集上的AUC值，注意prodict_prob_y输出的是预测为1的概率
test_auc = metrics.roc_auc_score(test_y, prodict_prob_y)
```

## 一些trick

### 过拟合问题

源自过多特征。

* 减少特征数量。
* 正则化。保留所有特征，但减少特征权重。L1、L2正则化线性模型一节已经介绍过。

### 其他优化算法

* Conjugate gradient method(共轭梯度法)
* Quasi-Newton method(拟牛顿法)
* BFGS method
* L-BFGS(Limited-memory BFGS)

后二者由拟牛顿法引申出来，与梯度下降算法相比，这些算法的优点是：

* 第一，不需要手动的选择步长；
* 第二，通常比梯度下降算法快；
* 但是缺点是更复杂。

实际上，为了提高算法收敛速度和节省内存，实际应用在迭代求解时往往会使用高效的优化算法，如LBFGS、信赖域算法等（著名的工具包LibLinear就是基于信赖域现的，Spark MLlib里的逻辑回归是基于LBFGS实现的）。但这些**求解方法是基于批量处理的，批处理算法无法高效处理超大规模的数据集，也无法对线上模型进行快速实时更新**。

随机梯度下降是相对于批处理的另外一种优化方法，它每次只用一个样本来更新模型的权重，这样就可以**更快地进行模型迭代**。对于广告和新闻推荐这种数据和样本更新比较频繁场景，快速的模型更新能够更早捕捉到新数据的规律进而提升业务指标。谷歌的FTRL就是基于随机梯度下降的一种逻辑回归优化算法。现在很多公司线上使用的是FTRL算法。FTRL算法是谷歌基于很多前人工作总结出来的。

### 多分类问题

如果一个样本只对应于一个标签，我们可以假设每个样本属于不同标签的概率服从于几何分布，使用多项逻辑回归（Softmax Regression）来进行分类。

当存在样本可能属于多个标签的情况时，我们可以训练**k个二分类**的逻辑回归分类器。第i个分类器用以区分每个样本是否可以归为第i类，训练该分类器时，需要把标签重新整理为“第i类标签”与“非第i类标签”两类。遇过这样的办法，我们就**解决了每个样本可能拥有多个标签的情况**。

分解为多个二分类问题。

一对一、一对多或者多对多。

## 逻辑回归和线性回归

首先逻辑回归处理的是分类问题，线性回归处理的是回归问题，这是两者的最本质的区别。

逻辑回归可以看作广义统性模型（Generalized Linear Models）；在因变量y服从二元分布时的一个特殊情况；而使用最小二乘法求解线性回归时，我们认为因变量y服从正态分布。

当然逻辑回归相结性回归也不乏相同之处，首先我们可以认为二者都使用了极大似然估计来对训练样本进行建模。另外，二者在求解超参数的过程中，都可以使用梯度下降的方法，这也是监督学习中一个常见的相似之处。

## 应用

逻辑回归常用于疾病自动诊断、经济预测、点击率预测等领域。由于其**处理速度快且容易并行**，逻辑回归适合用来学习需要大规模训练的样本和特征，对于广告十亿量级的特征和亿量级的特征来说，逻辑回归有着天然的优势，因而逻辑回归在工业界获得了广泛的应用。而逻辑回归的缺点是，**需要大量的特征组合和离散的工作来增加特征的表达性，模型表达能力弱，比较容易欠拟合**。

业界对逻辑回归的研究热点主要集中在**稀疏性、准确性和大规模计算**上。实际应用逻辑回归前，经常会对特征进行独热（One Hot）编码，比如广告点击率应用中的用户ID、广告ID。为了实现计算效率和性能的优化，逻辑回归求解有很多种优化方法，比如BFGS、LBFGS、共辄梯度法、信赖域法，其中前两个方法是牛顿法的变种，LBFGS算法是BFGS算法在受限内存限制下的近似优化。针对逻辑回归在线学习时遇到的稀疏性和准确性问题，谷歌和伯克利分校提出了稀疏性比较好的FOBOS算法、微软提出了RDA算法。谷歌综合了精度比较好的RDA和稀疏性比较好的FOBOS提出了FTRL，但在L1范数或者非光滑的正则项下，FTRL的效果会更好。

在实际应用中，逻辑回归也需要注意正则化的问题。L1正则（也称LASSO）假设模型参数取值满足拉普拉斯分布，L2正则（也称阳RIDGE）假设模型参数取值满足高斯分布。

## sklearn

```
sklearn.linear_model.LogisticRegression

penalty：惩罚项的种类，l1或l2。
C：惩罚系数，控制惩罚程度。
class_weight：考虑类不平衡，代价敏感。
```

## 参考

[机器学习总结之逻辑回归Logistic Regression](https://blog.csdn.net/hustlx/article/details/51153029)

[代码实现](https://github.com/SmirkCao/MLiA/blob/master/CH05%20Logistic%20Regression.ipynb)
