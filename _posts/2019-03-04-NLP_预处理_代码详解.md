---
layout: post
title: NLP预处理代码详解
categories: [NLP]
description: some word here
keywords: keyword1, keyword2
---

# 构造特征矩阵

[参考代码中构造特征矩阵的过程](https://github.com/gaussic/text-classification-cnn-rnn/blob/155e27b4dac6fedca9e5ed909fd2465a2d54ea74/run_cnn.py)

* build\_vocab(train\_dir, vocab\_dir, config\.vocab\_size) 构造词汇表，此时构造的词汇表示**字符级的表示**。
* words, word\_to\_id = read\_vocab(vocab\_dir) 读取上一步存储的词汇表，转换为\{词: id\}表示。
* x\_train, y\_train = process\_file(train\_dir, word\_to\_id, cat\_to\_id, config\.seq\_length) 将数据集从文字转换为固定长度的id序列表示，即构造特征矩阵。

[参考代码中一些函数的解释](https://github.com/gaussic/text-classification-cnn-rnn/blob/155e27b4dac6fedca9e5ed909fd2465a2d54ea74/data/cnews_loader.py)

## build\_vocab过程

* 首先read\_file读取相应的文件，然后将每行的label和content分开，记住这步contents\.append(list(native\_content(content)))，将content转为了list（不需要分词，直接采用字符级别特征）。此时返回的data\_train结构类似为：\[\[\],\[\],\[\]\]。
* 然后在build\_vocab里面利用一个for循环将all\_data.extend(content)，all\_data结构为\[,,,,\]。
* 然后利用Counter统计每个字符出现的频率，并且筛选（count\_pairs = counter\.most\_common(vocab\_size - 1)）（vocab\_size - 1的原因是词表中需要添加一个padding字符）。
* 词表存储open\_file(vocab\_dir, mode='w').write('\n'\.join(words) + '\n')，每个词后加一个换行符。

## read\_vocab过程

* 读取词表
* 转换为词到id的表示。利用dict(zip(words, range(len(words))))。

## process\_file过程

* 首先read\_file读取相应的文件
* 将contents中的每一个文本中的每一个字符转换为相应的id表示。data\_id.append(\[word\_to\_id\[x\] for x in contents\[i\] if x in word\_to\_id\])。到这步你已经得到了类似\[\[\],\[\],\[\]\]的结构。得出这个结构就ok了。
* 此时我们不需要后面的几行使用keras提供的pad\_sequences来将文本pad为固定长度。

# 一些说明

不管我们构造什么样的特征矩阵，我们最终要得到上述process\_file过程中第二步得到的结构（结构整体是一个list，list中的每个元素x也是list，x中的元素是字符id）。因为IMDB英文数据集已经是这种结构，所以我们不需要进行转换。

示例代码是将每篇文档转换为字符id表示的list，**我们的要求是转换为词id表示的list**。最主要的改动有：

* build\_vocab过程，第一步需要分词，统计词出现的频率，然后根据词出现的频率以及停用词表将某些词去掉。

**我们要求得到词级别的矩阵化**，主要改动有：

* build\_vocab过程，第一步需要分词，统计词出现的频率，然后根据词出现的频率以及停用词表将某些词去掉。
* process\_file过程的第二部分根据词表中是否出现某个词将我们的文档转换为string类型，以空格隔开词。
* 转换为特征向量。

参考代码如下。

```
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
 
if __name__ == "__main__":
    count = CountVectorizer()
    #定义一个文档数组
    docs = np.array([
        "The sun is shining",
        "The weather is sweet",
        "The sun is shining and the weather is sweet"
    ])
    #创建词袋模型的词汇库
    bag = count.fit_transform(docs)
    #查看词汇的位置，词汇是以字典的形式存储
    print(count.vocabulary_)
    #{'the': 5, 'sun': 3, 'is': 1, 'shining': 2, 'weather': 6, 'sweet': 4, 'and': 0}
    print(bag.toarray())
    '''
    [[0 1 1 1 0 1 0]
     [0 1 0 0 1 1 1]
     [1 2 1 1 1 2 1]]
```

