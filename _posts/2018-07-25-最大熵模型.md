---
layout: post
title: 最大熵模型
categories: [NLP]
description: 介绍最大熵模型
keywords: NLP
---

## 各种熵

熵：值越大，表示概率分布不确定性的期望值。这个值越大表示概率分布不确定性越大。它为我们人类提供的“信息”就越小，我们就越难利用这个概率分布来做出一个正确的判断。从这个角度，我们可以看到，熵是对概率分布信息含量的衡量，这与它是不确定性的衡量，其实是两种解读方式而已。

联合熵：联合概率分布的熵。H(X,Y)永远大于等于H(X)或H(Y)。仅当X没有不确定性时，比如永远是正面朝上，此时，在Y的基础上联合X，并没有引入新的不确定性，所以，H(x,y)=H(y)。

条件熵：H(x|y) = H(x,y) - H(y)。条件熵是在**y上引入x后增加的不确定性**。且增加的不确定性无论如何不能大于x本身的不确定性，即H(x|y) <= H(x)，仅当x、y相互独立时，等号才成立。

相对熵：又叫KL散度。d= -Σp(x)log(q(x))，D(q||p) = d - H(p)。相对熵来衡量我们通过计算得出的**真实分布的表达式q**，究竟与由样本统计得来的分布**p有多接近**，在衡量多接近这个概念时，我们运用到了熵的形式。

交叉熵：就是相对熵公式中的d。**交叉熵是不满足交换律**。

## 最大熵原理

最大熵模型是由最大熵原理推导实现的。

最大熵原理是概率模型学习的一个准则，**评价一个模型的好坏是根据熵的大小，熵大说明模型越好**。因此可以理解，最大熵原理就是满足一定的约束条件下，选择熵最大的模型。这样其实是解决在约束条件下的最优化问题求解。

计算最大熵根据两个前提去解决问题：

* 解决问题要满足一定约束。
* 不做任何假设，就是在约束外的事件发生概率为等概率。

## 最大熵模型

最大熵模型其实就是**最大熵原理应用到分类问题中**。

## 参考

[机器学习面试之各种混乱的熵](https://www.jianshu.com/p/09b70253c840)
